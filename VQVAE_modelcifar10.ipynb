{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\git\n"
     ]
    }
   ],
   "source": [
    "cd git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers # keras for tensorflow.keras   \n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VectorQuantizer(layers.Layer):\n",
    "    def __init__(self, num_embeddings, embedding_dim, beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # The `beta` parameter is best kept between [0.25, 2] as per the paper.\n",
    "        self.beta = beta\n",
    "\n",
    "        # Initialize the embeddings which we will quantize.\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=\"embeddings_vqvae\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate the input shape of the inputs and\n",
    "        # then flatten the inputs keeping `embedding_dim` intact.\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "\n",
    "        # Quantization.\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "\n",
    "        # Reshape the quantized values back to the original input shape\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer. You can learn more\n",
    "        # about adding losses to different layers here:\n",
    "        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n",
    "        # the original paper to get a handle on the formulation of the loss function.\n",
    "        commitment_loss = tf.reduce_mean((tf.stop_gradient(quantized) - x) ** 2)\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        self.add_loss(self.beta * commitment_loss + codebook_loss)\n",
    "\n",
    "        # Straight-through estimator.\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes.\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances.\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        return encoding_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resblock(x, kernelsize, filters):\n",
    "    fx = layers.Conv2D(filters, kernelsize, activation='relu', padding='same')(x)\n",
    "    fx = layers.BatchNormalization()(fx)\n",
    "    fx = layers.Conv2D(filters, kernelsize, padding='same')(fx)\n",
    "    out = layers.Add()([x,fx])\n",
    "    out = layers.ReLU()(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_encoder(latent_dim=16):\n",
    "    encoder_inputs = keras.Input(shape=(160, 160, 3))\n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "    x = resblock(x, 4, 32)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = resblock(x, 4, 64)\n",
    "    x = layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    encoder_outputs = layers.Conv2D(latent_dim, 1, padding=\"same\")(x)\n",
    "    return keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "\n",
    "def get_decoder(latent_dim=16):\n",
    "    latent_inputs = keras.Input(shape=get_encoder(latent_dim).output.shape[1:])\n",
    "    x = layers.Conv2DTranspose(128, 4, activation=\"relu\", strides=2, padding=\"same\")(latent_inputs) #dimesion of the filter 4x4 because in the decoding you jump a row each two and with 3x3 you evaluate only the zeros around your central pixel\n",
    "    x = resblock(x, 4, 128)\n",
    "    x = layers.Conv2DTranspose(64, 4, activation=\"relu\", strides=2, padding=\"same\")(x) \n",
    "    x = resblock(x, 4, 64)\n",
    "    x = resblock(x, 4, 64)\n",
    "    x = layers.Conv2DTranspose(32, 4, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(3, 3, padding=\"same\")(x)\n",
    "    return keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vqvae(latent_dim=16, num_embeddings=64):\n",
    "    vq_layer = VectorQuantizer(num_embeddings, latent_dim, name=\"vector_quantizer\")\n",
    "    encoder = get_encoder(latent_dim)\n",
    "    decoder = get_decoder(latent_dim)\n",
    "    inputs = keras.Input(shape=(160, 160, 3))\n",
    "    encoder_outputs = encoder(inputs)\n",
    "    quantized_latents = vq_layer(encoder_outputs)\n",
    "    reconstructions = decoder(quantized_latents)\n",
    "    return keras.Model(inputs, reconstructions, name=\"vq_vae\")\n",
    "\n",
    "\n",
    "get_vqvae().summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VQVAETrainer(keras.models.Model):\n",
    "    def __init__(self, train_variance, latent_dim=32, num_embeddings=128, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_variance = train_variance\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        self.vqvae = get_vqvae(self.latent_dim, self.num_embeddings)\n",
    "\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.vq_loss_tracker = keras.metrics.Mean(name=\"vq_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.vq_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, x):#provare a fare un fit con un fit normale\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Outputs from the VQ-VAE.\n",
    "            reconstructions = self.vqvae(x)\n",
    "\n",
    "            # Calculate the losses.\n",
    "            reconstruction_loss = (\n",
    "                tf.reduce_mean((x - reconstructions) ** 2) / self.train_variance\n",
    "            )\n",
    "            total_loss = reconstruction_loss + sum(self.vqvae.losses)\n",
    "\n",
    "        # Backpropagation.\n",
    "        grads = tape.gradient(total_loss, self.vqvae.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.vqvae.trainable_variables))\n",
    "\n",
    "        # Loss tracking.\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.vq_loss_tracker.update_state(sum(self.vqvae.losses))\n",
    "\n",
    "        # Log results.\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"vqvae_loss\": self.vq_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# data\n",
    "dataset_name = \"cifar10\"\n",
    "dataset_repetitions = 1\n",
    "image_size = 32\n",
    "batch_size = 32\n",
    "\n",
    "def preprocess_image(data):\n",
    "    # center crop image\n",
    "    height = tf.shape(data[\"image\"])[0]\n",
    "    width = tf.shape(data[\"image\"])[1]\n",
    "    crop_size = tf.minimum(height, width)\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        data[\"image\"],\n",
    "        (height - crop_size) // 2,\n",
    "        (width - crop_size) // 2,\n",
    "        crop_size,\n",
    "        crop_size,\n",
    "    )\n",
    "\n",
    "    # resize and clip\n",
    "    # for image downsampling it is important to turn on antialiasing\n",
    "    image = tf.image.resize(image, size=[image_size, image_size], antialias=True)\n",
    "    return tf.clip_by_value(image / 255.0, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def prepare_dataset(split):\n",
    "    # the validation dataset is shuffled as well, because data order matters\n",
    "    # for the KID estimation\n",
    "    return (\n",
    "        tfds.load(dataset_name, split=split, shuffle_files=True)\n",
    "        .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .repeat(dataset_repetitions)\n",
    "        .shuffle(10 * batch_size)\n",
    "        .batch(batch_size, drop_remainder=True)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "# load dataset\n",
    "x_train_scaled = prepare_dataset(\"train[:80%]+validation[:80%]+test[:80%]\")\n",
    "x_test_scaled = prepare_dataset(\"train[80%:]+validation[80%:]+test[80%:]\")\n",
    "#data_variance = np.var(x_train / 255.0) \n",
    "data_variance = 1 #random variance change it ???\n",
    "\n",
    "\n",
    "# Creates and compiles of the model\n",
    "vqvae_trainer = VQVAETrainer(data_variance, latent_dim=16, num_embeddings=128)\n",
    "vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "\n",
    "# creates the saves destinations for model and weights\n",
    "filename = './synthetic_data/checkpoints/model_cifar0.csv'\n",
    "checkpoint_file = './synthetic_data/checkpoints/model_cifar'\n",
    "load_checkpoint = './synthetic_data/checkpoints/model_cifar'\n",
    "i = 1\n",
    "while os.path.isfile(filename):\n",
    "    filename = f'./synthetic_data/checkpoints/model_cifar{i}.csv'\n",
    "    load_checkpoint = f'./synthetic_data/checkpoints/model_cifar{i-1}'\n",
    "    checkpoint_file = f'./synthetic_data/checkpoints/model_cifar{i}'\n",
    "    i += 1\n",
    "\n",
    "# training of the neural network and then saves as a .csv file the model parameters hystory\n",
    "params = vqvae_trainer.fit(x_train_scaled, epochs=15, batch_size=32)\n",
    "model_params = pd.DataFrame(params.history)\n",
    "model_params.to_csv(filename, index=False)# il file è pieno solo delle funzioni di loss ma non dei parametri del modella...???\n",
    "\n",
    "\n",
    "# load the weights in checkpoint format\n",
    "#vqvae_trainer.load_weights(load_checkpoint)\n",
    "\n",
    "\n",
    "# saves the weights of the training\n",
    "vqvae_trainer.save_weights(checkpoint_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# show the images and the codes for numpy dataset for 4 images\n",
    "def show_subplot(original, reconstructed):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original)# this for tensorflow dataset the following commented for numpy\n",
    "    #plt.imshow(original.squeeze() + 0.5)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(reconstructed)# this for tensorflow dataset the following commented for numpy\n",
    "    #plt.imshow(reconstructed.squeeze() + 0.5)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# with tensorflow dataset\n",
    "for example in x_test_scaled.take(1):\n",
    "  test_images = example[0:4]\n",
    "\n",
    "\n",
    "trained_vqvae_model = vqvae_trainer.vqvae\n",
    "'''\n",
    "idx = np.random.choice(len(x_test_scaled), 4)\n",
    "test_images = x_test_scaled[idx]\n",
    "'''\n",
    "reconstructions_test = trained_vqvae_model.predict(test_images)\n",
    "\n",
    "for test_image, reconstructed_image in zip(test_images, reconstructions_test):\n",
    "    show_subplot(test_image, reconstructed_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder = vqvae_trainer.vqvae.get_layer(\"encoder\")\n",
    "quantizer = vqvae_trainer.vqvae.get_layer(\"vector_quantizer\")\n",
    "\n",
    "encoded_outputs = encoder.predict(test_images)\n",
    "flat_enc_outputs = encoded_outputs.reshape(-1, encoded_outputs.shape[-1])\n",
    "codebook_indices = quantizer.get_code_indices(flat_enc_outputs)\n",
    "codebook_indices = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(test_images[i])# this for tensorflow dataset the following commented for numpy\n",
    "    #plt.imshow(test_images[i].squeeze() + 0.5)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(codebook_indices[i])\n",
    "    plt.title(\"Code\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
